import torch
from diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline, StableDiffusionInpaintPipelineLegacy, DDIMScheduler, AutoencoderKL
from PIL import Image

from ip_adapter import IPAdapterPlus

base_model_path = "SG161222/Realistic_Vision_V4.0_noVAE"
vae_model_path = "stabilityai/sd-vae-ft-mse"
image_encoder_path = "models/image_encoder"
ip_ckpt = "models/ip-adapter-plus_sd15.bin"
device = "cuda"

base_model_path = r"D:\Models\stable-diffusion-v1-5"
vae_model_path = "stabilityai/sd-vae-ft-mse"

use_BioCLIP = True
# image_encoder_path = r"StableDiffusionWeedV2\bioclip\model"
image_encoder_path = r'StableDiffusionWeedV2\bioclip\model\open_clip_pytorch_model.bin'

# image_encoder_path = r"D:\Models\clip_vision\clip_image_encoder"
# ip_ckpt = r"StableDiffusion\ComfyUI\models\ipadapter\ip_adapter_plus_sd15_10_species_bioclip_train120k.bin"
ip_ckpt = r"StableDiffusion\IP-Adapter\output_models_sd_ip_adapter__MultipleSpecies_10species\checkpoint-115000\ip_adapter.bin"
# ip_ckpt = r"StableDiffusion\ComfyUI\models\ipadapter\ip_adapter_plus_sd15_10_species.bin"
device = "cuda"

def image_grid(imgs, rows, cols):
    assert len(imgs) == rows*cols

    w, h = imgs[0].size
    grid = Image.new('RGB', size=(cols*w, rows*h))
    grid_w, grid_h = grid.size
    
    for i, img in enumerate(imgs):
        grid.paste(img, box=(i%cols*w, i//cols*h))
    return grid

noise_scheduler = DDIMScheduler(
    num_train_timesteps=1000,
    beta_start=0.00085,
    beta_end=0.012,
    beta_schedule="scaled_linear",
    clip_sample=False,
    set_alpha_to_one=False,
    steps_offset=1,
)
vae = AutoencoderKL.from_pretrained(vae_model_path).to(dtype=torch.float16)

# load SD pipeline
pipe = StableDiffusionPipeline.from_pretrained(
    base_model_path,
    torch_dtype=torch.float16,
    scheduler=noise_scheduler,
    vae=vae,
    feature_extractor=None,
    safety_checker=None,
    low_cpu_mem_usage=False
)

# read image prompt
# image = Image.open("assets/images/statue.png")
image = Image.open(r"D:\Dataset\WeedData\weed_10_species\train2017_real_object_in_box\_MultipleSpecies_10species_shuffle_high_quality/5_Ragweed_20210906_iPhoneSE_YL_222_227.jpg")
image.resize((256, 256))

# load ip-adapter
ip_model = IPAdapterPlus(pipe, image_encoder_path, ip_ckpt, device, num_tokens=16, use_BioCLIP=use_BioCLIP)

# only image prompt
# images = ip_model.generate(pil_image=image, num_samples=4, num_inference_steps=50, seed=42)
images = ip_model.generate(pil_image=image, num_samples=4, num_inference_steps=20, seed=42)
grid = image_grid(images, 1, 4)
grid


# multimodal prompts
# images = ip_model.generate(pil_image=image, num_samples=4, num_inference_steps=50, seed=42,
#         prompt="best quality, high quality, wearing a hat on the beach", scale=0.6)
images = ip_model.generate(pil_image=image, num_samples=4, num_inference_steps=20, seed=42,
        prompt="Please generate a top-down illustration of a field with a realistic depiction of plants under various lighting conditions", scale=0.6)
grid = image_grid(images, 1, 4)
grid
