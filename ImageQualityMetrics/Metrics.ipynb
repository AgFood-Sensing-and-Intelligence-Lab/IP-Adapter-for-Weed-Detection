{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skus = [\n",
    "    'Lambsquarters',\n",
    "    'PalmerAmaranth',\n",
    "    'Waterhemp',\n",
    "    'MorningGlory',\n",
    "    'Purslane',\n",
    "    'Goosegrass',\n",
    "    'Carpetweed',\n",
    "    'SpottedSpurge',\n",
    "    'Ragweed',\n",
    "    'Eclipta',\n",
    "]\n",
    "\n",
    "sku_dir_real_dict = {}\n",
    "sku_dir_real_hint_dict = {}\n",
    "\n",
    "for sku in skus:\n",
    "    sku_dir_real_dict[sku] = fr'D:\\BoyangDeng\\StableDiffusion\\ControlNet\\training\\weedall_origin_{sku}_block\\target'\n",
    "    sku_dir_real_hint_dict[sku] = fr'D:\\BoyangDeng\\StableDiffusion\\ControlNet\\training\\weedall_origin_{sku}_block\\source'\n",
    "skus = [\n",
    "    'Lambsquarters',\n",
    "    'Waterhemp',\n",
    "]\n",
    "for sku in skus:\n",
    "    sku_dir_real_dict[sku] = fr'D:\\BoyangDeng\\StableDiffusion\\ControlNet\\training\\weedall_crop1500_{sku}\\target'\n",
    "    sku_dir_real_hint_dict[sku] = fr'D:\\BoyangDeng\\StableDiffusion\\ControlNet\\training\\weedall_crop1500_{sku}\\source'\n",
    "skus = [\n",
    "    'Purslane',\n",
    "]\n",
    "for sku in skus:\n",
    "    sku_dir_real_dict[sku] = fr'D:\\BoyangDeng\\StableDiffusion\\ControlNet\\training\\weedall_crop1500_{sku}_block\\target'\n",
    "    sku_dir_real_hint_dict[sku] = fr'D:\\BoyangDeng\\StableDiffusion\\ControlNet\\training\\weedall_crop1500_{sku}_block\\source'\n",
    "\n",
    "sku_dir_fake_dict = {}\n",
    "for sku in sku_dir_real_dict:\n",
    "    sku_dir_fake_dict[sku] = fr'D:\\Dataset\\WeedData\\weed_10_species\\train_image_for_controlnet_all\\train2017_{sku}_detection'\n",
    "\n",
    "sku_dir_real_dict = dict(sorted(sku_dir_real_dict.items()))\n",
    "sku_dir_fake_dict = dict(sorted(sku_dir_fake_dict.items()))\n",
    "# sku_dir_real_dict, sku_dir_fake_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sku_dir_real_dict = {}\n",
    "sku_dir_synthetic_dict = {}\n",
    "sku_dir_real_dict['train'] = r'weed_10_species\\train2017_split_3_same_im_1024'\n",
    "sku_dir_synthetic_dict['train'] = r'weed_10_species\\train2017_split_3_generated_im_res512_script_edge_blend'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sku_dir_real_dict = {}\n",
    "sku_dir_synthetic_dict = {}\n",
    "\n",
    "sku_dir_real_dict['train'] = r'weed_10_species\\train2017_real_object_in_box'\n",
    "\n",
    "sku_dir_synthetic_dict['train'] = r'generated_10_species\\train2017_split_1_generated_im_res512_script_edge_blend_all_object_in_box'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skus = [\n",
    "    'Lambsquarters',\n",
    "    'PalmerAmaranth',\n",
    "    'Waterhemp',\n",
    "    'MorningGlory',\n",
    "    'Purslane',\n",
    "    'Goosegrass',\n",
    "    'Carpetweed',\n",
    "    'SpottedSpurge',\n",
    "    'Ragweed',\n",
    "    'Eclipta',\n",
    "]\n",
    "sku_dir_real_dict = {}\n",
    "sku_dir_synthetic_dict = {}\n",
    "for sku in skus:\n",
    "    sku_dir_real_dict[sku] = fr'weed_10_species\\train2017_real_object_in_box\\{sku}'\n",
    "for sku in sku_dir_real_dict:\n",
    "    sku_dir_synthetic_dict[sku] = fr'generated_10_species\\train2017_split_1_generated_im_res512_script_edge_blend_all_mask_ann_only_bioclip_object_in_box\\{sku}'\n",
    "sku_dir_real_hint_dict = sku_dir_real_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def calculate_psnr(img1, img2):\n",
    "    # img1 and img2 have range [0, 255]\n",
    "    img1 = img1.astype(np.float64)\n",
    "    img2 = img2.astype(np.float64)\n",
    "    mse = np.mean((img1 - img2)**2)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    return 20 * math.log10(255.0 / math.sqrt(mse))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(12.7626)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "_ = torch.manual_seed(123)\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "fid = FrechetInceptionDistance(feature=64)\n",
    "imgs_dist1 = torch.randint(0, 200, (10, 3, 299, 299), dtype=torch.uint8)\n",
    "imgs_dist2 = torch.randint(100, 255, (10, 3, 299, 299), dtype=torch.uint8)\n",
    "fid.update(imgs_dist1, real=True)\n",
    "fid.update(imgs_dist2, real=False)\n",
    "fid.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_dir(path, list_name, extension, return_names=False):\n",
    "    import os\n",
    "    for file in os.listdir(path):\n",
    "        file_path = os.path.join(path, file)\n",
    "        if os.path.isdir(file_path):\n",
    "            list_dir(file_path, list_name, extension)\n",
    "        else:\n",
    "            if file_path.endswith(extension):\n",
    "                if return_names:\n",
    "                    list_name.append(file)\n",
    "                else:\n",
    "                    list_name.append(file_path)\n",
    "    try:\n",
    "        list_name = sorted(list_name, key=lambda k: int(os.path.split(k)[1].split(extension)[0].split('_')[-1]))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return list_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FID for each sku / Overall FID\n",
    "\"\"\"\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import random\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def preprocess_image1(image):\n",
    "    image = torch.tensor(image).unsqueeze(0)\n",
    "    # image = image.permute(0, 3, 1, 2) / 255.0\n",
    "    image = image.permute(0, 3, 1, 2)\n",
    "    return image\n",
    "compute_each_sku = False\n",
    "fid = FrechetInceptionDistance(feature=64, normalize=True)\n",
    "sku_FID_dict = {}\n",
    "for sku in sku_dir_real_dict:\n",
    "    print(sku)\n",
    "    src_dir = sku_dir_real_dict[sku]\n",
    "    src_hint_dir = sku_dir_real_hint_dict[sku]\n",
    "\n",
    "    dst_dir = sku_dir_synthetic_dict[sku]\n",
    "    print(src_dir)\n",
    "    print(dst_dir)\n",
    "    # fid = FrechetInceptionDistance(feature=64, normalize=False)\n",
    "    if compute_each_sku:\n",
    "        fid = FrechetInceptionDistance(feature=64, normalize=True)\n",
    "\n",
    "    update_step = 100\n",
    "    src_im_paths = []\n",
    "    src_ann_paths = list_dir(src_hint_dir, [], '.jpg')\n",
    "    for i_im, src_ann_path in tqdm(enumerate(src_ann_paths)):\n",
    "            im_name_base = src_ann_path.split('.')[0]\n",
    "            _, im_name = os.path.split(src_ann_path)\n",
    "            im_name = im_name_base+'.jpg'\n",
    "            src_im_path = os.path.join(src_dir, im_name)\n",
    "            src_im_paths.append(src_im_path)\n",
    "\n",
    "    dst_im_paths = []\n",
    "    dst_ann_paths = list_dir(dst_dir, [], '.jpg')\n",
    "    for i_im, dst_ann_path in tqdm(enumerate(dst_ann_paths)):\n",
    "            im_name_base = dst_ann_path.split('.')[0]\n",
    "            im_dir, im_name = os.path.split(dst_ann_path)\n",
    "            im_name = im_name_base+'.jpg'\n",
    "            dst_im_path = os.path.join(im_dir, im_name)\n",
    "            dst_im_paths.append(dst_im_path)\n",
    "\n",
    "    random.seed(1)\n",
    "    random.shuffle(src_im_paths)\n",
    "    random.shuffle(dst_im_paths)\n",
    "    im_num = -1\n",
    "    # im_num = 1000\n",
    "    # im_num = min(len(src_im_paths), len(dst_im_paths))\n",
    "    im_size = 512\n",
    "    original_images = []\n",
    "    for i_im, src_im_path in tqdm(enumerate(src_im_paths)):\n",
    "        if im_num > 0 and i_im>=im_num:\n",
    "            if len(original_images) > 0:\n",
    "                original_images=torch.cat([preprocess_image1(image) for image in original_images])\n",
    "                fid.update(original_images, real=True)\n",
    "                original_images = []\n",
    "            break\n",
    "        original_image=cv2.imread(src_im_path)\n",
    "        original_image = cv2.resize(original_image, (im_size,im_size))\n",
    "        original_images.append(original_image)\n",
    "        if i_im%update_step==0 or i_im == len(src_im_paths)-1:\n",
    "            original_images=torch.cat([preprocess_image1(image) for image in original_images])\n",
    "            fid.update(original_images, real=True)\n",
    "            original_images = []\n",
    "\n",
    "    edited_images =[]\n",
    "    for i_im, dst_im_path in tqdm(enumerate(dst_im_paths)):\n",
    "        if im_num > 0 and i_im>=im_num:\n",
    "            if len(edited_images) > 0:\n",
    "                edited_images=torch.cat([preprocess_image1(image) for image in edited_images])\n",
    "                fid.update(edited_images, real=False)\n",
    "                edited_images =[]\n",
    "            break\n",
    "        edited_image=cv2.imread(dst_im_path)\n",
    "        edited_image = cv2.resize(edited_image, (im_size,im_size))\n",
    "        edited_images.append(edited_image)\n",
    "        if i_im%update_step==0 or i_im == len(dst_im_paths)-1:\n",
    "            edited_images=torch.cat([preprocess_image1(image) for image in edited_images])\n",
    "            fid.update(edited_images, real=False)\n",
    "            edited_images =[]\n",
    "    if compute_each_sku:\n",
    "        FID = float(fid.compute())\n",
    "        print(f\"FID: {round(FID, 2)}\")\n",
    "        sku_FID_dict[sku] = round(FID, 2)\n",
    "    # original_images=torch.cat([preprocess_image1(image) for image in original_images])\n",
    "    # edited_images=torch.cat([preprocess_image1(image) for image in edited_images])\n",
    "    # # All images will be resized to 299 x 299 in official doc\n",
    "    # fid.update(original_images, real=True)\n",
    "    # fid.update(edited_images, real=False)\n",
    "if not compute_each_sku:\n",
    "    FID = float(fid.compute())\n",
    "    print(f\"FID: {round(FID, 2)}\")\n",
    "print(sku_FID_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import random\n",
    "from torchmetrics.image.inception import InceptionScore\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "_ = torch.manual_seed(123)\n",
    "from torchmetrics.image.inception import InceptionScore\n",
    "inception = InceptionScore()\n",
    "# generate some images\n",
    "imgs = torch.randint(0, 255, (100, 3, 299, 299), dtype=torch.uint8)\n",
    "inception.update(imgs)\n",
    "inception.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambsquarters\n",
      "D:\\BoyangDeng\\StableDiffusion\\test\\generated_10_species\\train2017_split_1_generated_im_res512_script_edge_blend_all_mask_ann_only_bioclip_object_in_box\\Lambsquarters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "541it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\BoyangDeng\\StableDiffusion\\test\\generated_10_species\\train2017_split_1_generated_im_res512_script_edge_blend_all_mask_ann_only_bioclip_object_in_box\\Lambsquarters\n",
      "541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "541it [00:38, 13.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PalmerAmaranth\n",
      "D:\\BoyangDeng\\StableDiffusion\\test\\generated_10_species\\train2017_split_1_generated_im_res512_script_edge_blend_all_mask_ann_only_bioclip_object_in_box\\PalmerAmaranth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "872it [00:00, 83212.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\BoyangDeng\\StableDiffusion\\test\\generated_10_species\\train2017_split_1_generated_im_res512_script_edge_blend_all_mask_ann_only_bioclip_object_in_box\\PalmerAmaranth\n",
      "872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "872it [01:02, 13.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waterhemp\n",
      "D:\\BoyangDeng\\StableDiffusion\\test\\generated_10_species\\train2017_split_1_generated_im_res512_script_edge_blend_all_mask_ann_only_bioclip_object_in_box\\Waterhemp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "741it [00:00, 65262.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\BoyangDeng\\StableDiffusion\\test\\generated_10_species\\train2017_split_1_generated_im_res512_script_edge_blend_all_mask_ann_only_bioclip_object_in_box\\Waterhemp\n",
      "741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "741it [00:53, 13.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MorningGlory\n",
      "D:\\BoyangDeng\\StableDiffusion\\test\\generated_10_species\\train2017_split_1_generated_im_res512_script_edge_blend_all_mask_ann_only_bioclip_object_in_box\\MorningGlory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "923it [00:00, 47284.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\BoyangDeng\\StableDiffusion\\test\\generated_10_species\\train2017_split_1_generated_im_res512_script_edge_blend_all_mask_ann_only_bioclip_object_in_box\\MorningGlory\n",
      "923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "923it [01:04, 14.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purslane\n",
      "D:\\BoyangDeng\\StableDiffusion\\test\\generated_10_species\\train2017_split_1_generated_im_res512_script_edge_blend_all_mask_ann_only_bioclip_object_in_box\\Purslane\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "677it [00:00, 64353.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\BoyangDeng\\StableDiffusion\\test\\generated_10_species\\train2017_split_1_generated_im_res512_script_edge_blend_all_mask_ann_only_bioclip_object_in_box\\Purslane\n",
      "677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "677it [00:47, 14.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goosegrass\n",
      "D:\\BoyangDeng\\StableDiffusion\\test\\generated_10_species\\train2017_split_1_generated_im_res512_script_edge_blend_all_mask_ann_only_bioclip_object_in_box\\Goosegrass\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1862it [00:00, 47959.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\BoyangDeng\\StableDiffusion\\test\\generated_10_species\\train2017_split_1_generated_im_res512_script_edge_blend_all_mask_ann_only_bioclip_object_in_box\\Goosegrass\n",
      "1862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1862it [02:09, 14.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carpetweed\n",
      "D:\\BoyangDeng\\StableDiffusion\\test\\generated_10_species\\train2017_split_1_generated_im_res512_script_edge_blend_all_mask_ann_only_bioclip_object_in_box\\Carpetweed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1016it [00:00, 61070.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\BoyangDeng\\StableDiffusion\\test\\generated_10_species\\train2017_split_1_generated_im_res512_script_edge_blend_all_mask_ann_only_bioclip_object_in_box\\Carpetweed\n",
      "1016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1016it [01:11, 14.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpottedSpurge\n",
      "D:\\BoyangDeng\\StableDiffusion\\test\\generated_10_species\\train2017_split_1_generated_im_res512_script_edge_blend_all_mask_ann_only_bioclip_object_in_box\\SpottedSpurge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1095it [00:00, 64127.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\BoyangDeng\\StableDiffusion\\test\\generated_10_species\\train2017_split_1_generated_im_res512_script_edge_blend_all_mask_ann_only_bioclip_object_in_box\\SpottedSpurge\n",
      "1095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1095it [01:17, 14.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ragweed\n",
      "D:\\BoyangDeng\\StableDiffusion\\test\\generated_10_species\\train2017_split_1_generated_im_res512_script_edge_blend_all_mask_ann_only_bioclip_object_in_box\\Ragweed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1134it [00:00, 67782.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\BoyangDeng\\StableDiffusion\\test\\generated_10_species\\train2017_split_1_generated_im_res512_script_edge_blend_all_mask_ann_only_bioclip_object_in_box\\Ragweed\n",
      "1134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1134it [01:20, 14.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eclipta\n",
      "D:\\BoyangDeng\\StableDiffusion\\test\\generated_10_species\\train2017_split_1_generated_im_res512_script_edge_blend_all_mask_ann_only_bioclip_object_in_box\\Eclipta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3303it [00:00, 65147.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\BoyangDeng\\StableDiffusion\\test\\generated_10_species\\train2017_split_1_generated_im_res512_script_edge_blend_all_mask_ann_only_bioclip_object_in_box\\Eclipta\n",
      "3303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3303it [03:52, 14.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IS: (tensor(3.7450), tensor(0.1136))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Overall / each sku IS\n",
    "\"\"\"\n",
    "import torch\n",
    "_ = torch.manual_seed(123)\n",
    "from torchmetrics.image.inception import InceptionScore\n",
    "# inception = InceptionScore()\n",
    "inception = InceptionScore(normalize=True)\n",
    "\n",
    "imgs =[]\n",
    "update_step = 100\n",
    "compute_each_sku = False\n",
    "# for sku in sku_dir_fake_dict:\n",
    "for sku in sku_dir_synthetic_dict:\n",
    "    print(sku)\n",
    "    dst_dir = sku_dir_synthetic_dict[sku]\n",
    "    print(dst_dir)\n",
    "    dst_im_paths = []\n",
    "    if compute_each_sku:\n",
    "        inception = InceptionScore(normalize=True)\n",
    "    # dst_ann_paths = list_dir(dst_dir, [], '.xml')\n",
    "    dst_ann_paths = list_dir(dst_dir, [], '.jpg')\n",
    "    for i_im, dst_ann_path in tqdm(enumerate(dst_ann_paths)):\n",
    "            im_name_base = dst_ann_path.split('.')[0]\n",
    "            im_dir, im_name = os.path.split(dst_ann_path)\n",
    "            im_name = im_name_base+'.jpg'\n",
    "            # dst_im_path = os.path.join(im_dir, im_name)\n",
    "            dst_im_path = dst_ann_path\n",
    "            dst_im_paths.append(dst_im_path)\n",
    "\n",
    "    print(dst_dir)\n",
    "    print(len(dst_im_paths))\n",
    "    # random.seed(0)\n",
    "    random.seed(1)\n",
    "    random.shuffle(dst_im_paths)\n",
    "    # im_num = 1000\n",
    "    im_num = len(dst_im_paths)\n",
    "    im_size = 512\n",
    "    for i_im, dst_im_path in tqdm(enumerate(dst_im_paths)):\n",
    "        if i_im>=im_num:\n",
    "            imgs=torch.cat([preprocess_image1(image) for image in imgs])\n",
    "            inception.update(imgs)\n",
    "            imgs = []\n",
    "            break\n",
    "        img=cv2.imread(dst_im_path)\n",
    "        img = cv2.resize(img, (im_size,im_size))\n",
    "        imgs.append(img)\n",
    "        if i_im%update_step==0 or i_im == len(dst_im_paths)-1:\n",
    "            imgs=torch.cat([preprocess_image1(image) for image in imgs])\n",
    "            inception.update(imgs)\n",
    "            imgs = []\n",
    "    if compute_each_sku:\n",
    "        print('IS:', inception.compute())\n",
    "if not compute_each_sku:\n",
    "    print('IS:', inception.compute())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved Pre and Rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from improved_precision_recall import IPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sku in sku_dir_fake_dict:\n",
    "    print(sku)\n",
    "    print(sku_dir_real_dict[sku])\n",
    "    print(sku_dir_fake_dict[sku])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading vgg16 for improved precision and recall...done\n",
      "WARNING: num_found_images(517) < num_samples(5000)\n",
      "WARNING: num_found_images(1200) < num_samples(5000)\n",
      "precision: 0.0033333333333333335\n",
      "recall: 0.27852998065764023\n",
      "found 1 images in D:\\BoyangDeng\\StableDiffusion\\ControlNet\\training\\weedall_origin_PalmerAmaranth_block\\target\n",
      "realism of first image in real: 2750.8298517469375\n",
      "found 1 images in D:\\Dataset\\WeedData\\weed_10_species\\train_image_for_controlnet\\train2017_PalmerAmaranth_detection\n",
      "realism of first image in fake: 0.6812681117764692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AFSALab\\.conda\\envs\\pytorch\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\AFSALab\\.conda\\envs\\pytorch\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "\n",
      "extracting features of 517 images:   0%|          | 0/11 [00:00<?, ?it/s]\n",
      "extracting features of 517 images:   9%|▉         | 1/11 [01:43<17:11, 103.15s/it]\n",
      "extracting features of 517 images:  18%|█▊        | 2/11 [01:43<06:24, 42.75s/it] \n",
      "extracting features of 517 images:  36%|███▋      | 4/11 [01:43<01:51, 15.99s/it]\n",
      "extracting features of 517 images:  55%|█████▍    | 6/11 [01:43<00:42,  8.47s/it]\n",
      "extracting features of 517 images:  73%|███████▎  | 8/11 [01:44<00:15,  5.09s/it]\n",
      "extracting features of 517 images:  91%|█████████ | 10/11 [01:54<00:05,  5.08s/it]\n",
      "extracting features of 517 images: 100%|██████████| 11/11 [01:55<00:00, 10.46s/it]\n",
      "\n",
      "extracting features of 1200 images:   0%|          | 0/24 [00:00<?, ?it/s]\n",
      "extracting features of 1200 images:   4%|▍         | 1/24 [00:20<07:54, 20.64s/it]\n",
      "extracting features of 1200 images:  12%|█▎        | 3/24 [00:20<01:53,  5.40s/it]\n",
      "extracting features of 1200 images:  21%|██        | 5/24 [00:27<01:20,  4.23s/it]\n",
      "extracting features of 1200 images:  25%|██▌       | 6/24 [00:27<00:56,  3.13s/it]\n",
      "extracting features of 1200 images:  29%|██▉       | 7/24 [00:27<00:39,  2.31s/it]\n",
      "extracting features of 1200 images:  38%|███▊      | 9/24 [00:27<00:19,  1.32s/it]\n",
      "extracting features of 1200 images:  42%|████▏     | 10/24 [00:27<00:14,  1.05s/it]\n",
      "extracting features of 1200 images:  50%|█████     | 12/24 [00:27<00:07,  1.54it/s]\n",
      "extracting features of 1200 images:  58%|█████▊    | 14/24 [00:30<00:09,  1.06it/s]\n",
      "extracting features of 1200 images:  62%|██████▎   | 15/24 [00:30<00:07,  1.26it/s]\n",
      "extracting features of 1200 images:  71%|███████   | 17/24 [00:35<00:09,  1.39s/it]\n",
      "extracting features of 1200 images:  79%|███████▉  | 19/24 [00:35<00:04,  1.06it/s]\n",
      "extracting features of 1200 images:  88%|████████▊ | 21/24 [00:36<00:02,  1.49it/s]\n",
      "extracting features of 1200 images:  92%|█████████▏| 22/24 [00:38<00:02,  1.05s/it]\n",
      "extracting features of 1200 images: 100%|██████████| 24/24 [00:38<00:00,  1.42it/s]\n",
      "extracting features of 1200 images: 100%|██████████| 24/24 [00:39<00:00,  1.66s/it]\n",
      "\n",
      "computing precision...:   0%|          | 0/1200 [00:00<?, ?it/s]\n",
      "computing precision...: 100%|██████████| 1200/1200 [00:00<00:00, 119982.95it/s]\n",
      "\n",
      "computing recall...:   0%|          | 0/517 [00:00<?, ?it/s]\n",
      "computing recall...: 100%|██████████| 517/517 [00:00<00:00, 51701.28it/s]\n",
      "\n",
      "extracting features of 1 images:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "extracting features of 1 images: 100%|██████████| 1/1 [00:00<00:00,  1.05it/s]\n",
      "extracting features of 1 images: 100%|██████████| 1/1 [00:00<00:00,  1.05it/s]\n",
      "\n",
      "extracting features of 1 images:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "extracting features of 1 images: 100%|██████████| 1/1 [00:00<00:00,  1.69it/s]\n",
      "extracting features of 1 images: 100%|██████████| 1/1 [00:00<00:00,  1.69it/s]\n"
     ]
    }
   ],
   "source": [
    "! python improved_precision_recall.py D:\\BoyangDeng\\StableDiffusion\\ControlNet\\training\\weedall_origin_PalmerAmaranth_block\\target D:\\Dataset\\WeedData\\weed_10_species\\train_image_for_controlnet\\train2017_PalmerAmaranth_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipr = IPR(args.batch_size, args.k, args.num_samples)\n",
    "ipr.compute_manifold_ref(args.path_real)  \n",
    "metric = ipr.precision_and_recall(images)\n",
    "print('precision =', metric.precision)\n",
    "print('recall =', metric.recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchmetrics.multimodal.clip_score import CLIPScore\n",
    "metric = CLIPScore(model_name_or_path=\"openai/clip-vit-base-patch16\")\n",
    "score = metric(torch.randint(255, (3, 224, 224), generator=torch.manual_seed(42)), \"a photo of a cat\")\n",
    "score.detach()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
